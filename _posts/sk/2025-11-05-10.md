---
title:  "Rookies 10일차: Python 데이터(numpy, pandas etc) 3"
excerpt: "파이썬 데이터 분석 심화: Pandas/NumPy 기반 데이터 조작 및 GroupBy"

categories: sk
tags:
  - [python, security, study]

typora-root-url: ../../
 
date: 2025-11-05
last_modified_at: 2025-11-05
published: true
---

**☀️<u>공지 사항</u>☀️** 해당 게시글은 `SK 쉴더스 루키즈, 생성형AI 활용 사이버보안 전문인력 양성과정` 을 수업을 듣고 정리한 글입니다. 자세한 교육 정보는  [SK 쉴더스 홈페이지](https://sslc.kr/) 를 확인해주세요.
{: .notice--danger}

# # 학습 내용 요약 (25/11/05) 

지난 학습 과정은 데이터 분석의 핵심적인 두 축인 **기초(NumPy/Pandas 구조)**을 종합적으로 다루었습니다. 오늘 학습은 어제 배운 개념에서 이어지는 내용입니다.


## 1. 개발 환경 구축 및 Pandas 핵심 구조 복습

효율적인 데이터 분석 프로젝트를 위해 정식 개발 환경을 준비하고 Pandas의 기본 구조를 체계적으로 이해하는 것이 중요합니다.

### 1.1. 추천 개발 환경 및 도구

향후 프로젝트 배포와 버전 관리를 고려하여 VS Code를 주 개발 환경으로 설정합니다.

| 도구 | 목적 | 비고 |
| :--- | :--- | :--- |
| **VS Code** | 주 개발 환경 (편집기) | 정식 파이썬 파일(`.py`) 개발 환경 구축에 최적 |
| **Git/GitHub** | 버전 관리 | 협업 및 작업 이력 관리 |
| **Streamlit** | 시각화 배포 | Matplotlib/Seaborn 결과물을 웹 기반 대시보드로 쉽게 전환 |

### 1.2. Pandas 핵심 데이터 구조: Series와 DataFrame

모든 데이터 조작의 근간이 되는 두 가지 구조를 다시 확인합니다.

| 구조 | 특징 | 역할 |
| :--- | :--- | :--- |
| **Series** | 1차원 데이터 타입 (인덱스 + 값) | DataFrame의 **열(Column)**을 구성하는 기본 단위 |
| **DataFrame** | 2차원 데이터 타입 (행과 열) | 여러 개의 Series가 합쳐진 형태, 분석의 기본 단위 |

---

## 2. 고성능 연산의 핵심: 축(Axis) 개념과 NumPy 활용

DataFrame의 내부 데이터는 NumPy 배열(행렬)로 처리되며, 대용량 데이터를 고속으로 처리하기 위해 축(Axis) 개념을 이해하는 것이 필수적입니다.

### 2.1. NumPy 축(Axis) 방향 이해

`sum()`, `mean()`, `drop()` 등 대부분의 연산 함수에서 `axis`를 지정하여 연산 방향을 결정합니다.

| 축 값 | 방향 | 연산 기준 | 목적 예시 |
| :---: | :---: | :---: | :--- |
| **0** | 세로 방향 | **열(Column)** 기준 연산 | 과목별 평균, 특정 열 삭제 (`axis=0`은 행 삭제) |
| **1** | 가로 방향 | **행(Row)** 기준 연산 | 학생별 총점/평균, 특정 열 삭제 (`axis=1`은 열 삭제) |

### 2.2. NumPy를 활용한 행별 평균 계산 및 열 추가

`DataFrame.values`를 사용하여 내부 NumPy 배열을 추출하면, `axis=1`을 지정하여 행 단위(학생별) 평균을 쉽게 계산할 수 있습니다.

**예시 코드 (학생별 평균 계산 및 정수형 변환):**

```python
# frm.values로 NumPy 배열 추출 후, 축 1(가로 방향)으로 평균 계산
# .astype(int) 또는 .astype(np.int32)를 사용하여 정수형으로 변환
frm['mean'] = np.mean(frm.values, axis = 1).astype(np.int32)
# print(frm) 
# 결과: 각 학생의 평균 점수가 'mean' 컬럼으로 추가됨
```

---

## 3. 데이터 로드 및 초기 탐색 (전처리 시작)

실제 분석에 들어가기 앞서 데이터의 품질과 구조를 정확히 파악해야 합니다. 우리는 Titanic 데이터셋을 사용합니다.

### 3.1. 외부 데이터셋 로드 및 인코딩 처리

Seaborn은 연습용 데이터셋을 쉽게 로드하는 데 유용하며, 실제 CSV 파일을 사용할 경우 인코딩 문제를 주의해야 합니다.

**예시 코드 (타이타닉 데이터 로드):**

```python
import seaborn as sns 
titanicRawData = sns.load_dataset('titanic')

# CSV 로드 시 인코딩 문제 대처 예시
# df_frm = pd.read_csv('file.csv', encoding='CP949') 
```

### 3.2. 데이터 품질 및 구조 확인 (결측치 점검)

분석 전 `info()`와 `describe()`는 필수 점검 과정입니다.

| 필수 탐색 함수 | 목적 | 핵심 보안/전처리 관점 |
| :--- | :--- | :--- |
| **`df.info()`** | 데이터 구조 및 결측값 확인 | `Object` 타입은 AI 학습에 사용 불가, 반드시 수치형으로 **전처리** 필요. Non-Null Count가 적으면 **결측값 처리** 필수. |
| **`df.describe()`** | 기술 통계 요약 | 숫자형 데이터의 분포(평균, 사분위수)를 확인하여 **이상치(Outlier)** 존재 가능성을 파악. |
| **`df['Col'].value_counts()`** | 값별 빈도수 확인 | 카테고리형 데이터의 편향성 또는 주요 범주 파악. |

---

## 4. 데이터 조작 마스터하기: 인덱싱, 필터링 및 서브셋 관리


### 4.1. Label 기반 데이터 수정 및 접근: `.LOC`

`.loc`는 행 레이블과 열 레이블(이름)을 기반으로 데이터를 접근하거나 수정할 때 사용됩니다.

**예시 코드 (데이터 수정):**

```python
# 1. '최호준' 학생의 'eng' 점수를 90으로 업데이트
frm.loc['최호준', 'eng'] = 90 

# 2. 평균 재계산
frm['mean'] = np.mean(frm.values, axis = 1).astype(np.int32)

# 3. 레이블 슬라이싱 (끝 레이블 포함)
# '임종석'부터 '이현우'까지, 'ko'부터 'eng'까지 추출
subset_loc = frm.loc['임종석':'이현우', 'ko':'eng'] 
```

### 4.2. 조건부 데이터 필터링 (불리안 인덱싱)

**⭐ 데이터 안정성 강조:** 원본 데이터를 직접 수정하기보다는 항상 조건을 적용하여 **서브셋**을 생성하는 방식으로 작업해야 합니다.

| 논리 연산자 | Pandas/Numpy (비트 연산자) |
| :---: | :---: |
| AND | **`&`** (앰퍼샌드) |
| OR | **`|`** (파이프) |
| NOT | **`~`** (틸드) |

**예시 코드 (복합 조건 필터링):**

타이타닉 데이터에서 `60세 이상 & 1등급 선실 & 여성`인 승객만 추출합니다.

```python
# 조건 정의
age_cond = (titanicRawData['age'] >= 60)
pclass_cond = (titanicRawData['pclass'] == 1)
sex_cond = (titanicRawData['sex'] == 'female')

# '&'를 사용하여 세 가지 조건을 모두 만족하는 서브셋 생성
subset_data = titanicRawData[age_cond & pclass_cond & sex_cond]
# print(subset_data.shape) # 조건을 만족하는 소수 인원만 추출됨
```

### 4.3. 인덱스 관리: 재설정 및 정리

데이터를 필터링하여 서브셋을 만들면 인덱스가 원본을 따라 불규칙하게 배열됩니다. 이를 해결하기 위해 `reset_index()`를 사용합니다.

**예시 코드 (인덱스 재설정 및 불필요 컬럼 삭제):**

```python
# 1. 인덱스 재설정 (inplace=True로 원본 반영)
subset_data.reset_index(inplace=True) 

# 2. reset_index()로 인해 자동으로 추가된 기존 'index' 컬럼 삭제
# 반드시 axis=1 (컬럼 방향) 지정
subset_data.drop(columns=['index'], axis=1, inplace=True) 
```

### 4.4. 효율적인 열 이름 변경

컬럼 이름이 많거나 일괄적인 규칙으로 변경해야 할 때, 딕셔너리 컴프리헨션을 사용하여 자동화할 수 있습니다.

**예시 코드 (모든 컬럼명을 소문자로 변경):**

```python
# 기존 컬럼명(c)을 순회하며 소문자로 변경(c.lower())하는 매핑 딕셔너리 생성
new_column_map = {c: c.lower() for c in subset_data.columns} 

# rename 함수에 딕셔너리를 전달하여 일괄 변경
subset_data.rename(columns=new_column_map, inplace=True)
```

---

## 5. GroupBy 및 Aggregation

**GroupBy**는 복잡한 데이터셋에서 특정 기준에 따른 통계적 인사이트를 도출하는 핵심 기술입니다. 이는 데이터 분석 과정에서 가장 강력하게 활용됩니다.

### 5.1. GroupBy의 3단계 절차: Split-Apply-Combine

| 단계 | 역할 | 주요 함수 |
| :--- | :--- | :--- |
| **Split (분할)** | 데이터를 지정된 키(컬럼)에 따라 그룹으로 나눔 | `DataFrame.groupby('Key')` |
| **Apply (적용)** | 각 그룹에 통계 함수, 계산, 또는 람다식을 적용 | `.mean()`, `.sum()`, `.count()` |
| **Combine (결합)** | 처리된 그룹 결과를 하나의 새로운 DataFrame으로 합침 | 집계(Aggregation) 후 자동 결합 |

### 5.2. GroupBy 객체 생성 및 특정 그룹 추출

`groupby()`를 실행하면 **DataFrameGroupBy 객체**가 반환되며, 이 객체에 집계 함수를 적용해야 결과물이 나옵니다.

**예시 코드 (특정 그룹의 데이터프레임 추출):**

```python
# Titanic 데이터에서 선실 등급(pclass)을 기준으로 그룹화
grouped = titanicRawData.groupby('pclass') 

# 1등급 선실(1) 승객 데이터만 DataFrame으로 추출
pclass_one_data = grouped.get_group(1)
# print(pclass_one_data.head()) 
```

### 5.3. 다중 집계 함수 (Aggregation) 활용

`agg()` 함수를 사용하여 그룹화된 데이터에 여러 개의 통계 함수를 동시에 적용할 수 있습니다. 이는 한 번의 연산으로 다양한 요약 통계를 얻을 수 있게 해줍니다.

**예시 코드 (선실 등급별 나이 및 요금 집계):**

```python
# 선실 등급별(pclass)로 나이(age)의 평균과 요금(fare)의 합계를 동시에 계산
summary_data = titanicRawData.groupby('pclass').agg({
    'age': ['mean', 'min', 'max'],    # 나이는 평균, 최소, 최대
    'fare': 'sum'                     # 요금은 합계
})

# print(summary_data)
# 결과: pclass (행)에 따라 age와 fare의 여러 통계치가 포함된 MultiIndex 컬럼의 DataFrame 반환
```
