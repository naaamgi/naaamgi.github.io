---
title: "Rookies 13일차: Python - LLM, Langchain, RAG 1"
excerpt: "생성형 AI의 핵심 개념, RAG 구조, 임베딩에 대해 자세히 알아보자."

categories: python
tags:
  - [python, security, study]

typora-root-url: ../../
 
date: 2025-11-10
last_modified_at: 2025-11-10
published: true
---

**☀️<u>공지 사항</u>☀️** 해당 게시글은 `SK 쉴더스 루키즈, 생성형AI 활용 사이버보안 전문인력 양성과정` 을 수업을 듣고 정리한 글입니다. 자세한 교육 정보는  [SK 쉴더스 홈페이지](https://sslc.kr/) 를 확인해주세요.
{: .notice--danger}


# # 학습 내용 요약 (25/11/10) 
## 1. 인공지능과 생성형 AI의 개념 및 발전

### 1.1. 인공지능, 머신러닝, 딥러닝의 관계

인공지능(AI)은 인간의 지능을 모방하는 컴퓨터 시스템을 의미합니다. AI의 하위 개념으로 머신러닝(ML)이 있으며, 이는 데이터를 기반으로 패턴을 학습하고 예측하는 기술입니다. 머신러닝의 한 분야인 딥러닝(DL)은 인공신경망을 활용하여 복잡한 데이터(이미지, 음성 등)를 처리합니다.

- **AI**: 인간의 사고와 행동을 모방하는 모든 기술을 포함합니다.
- **머신러닝(ML)**: 데이터에서 패턴을 학습하여 예측이나 분류를 수행합니다.
- **딥러닝(DL)**: 인공신경망을 활용하여 복잡한 데이터 구조를 학습합니다.

딥러닝의 대표적 신경망 구조로는 이미지 처리에 특화된 CNN(Convolutional Neural Network), 순차적 데이터에 적합한 RNN(Recurrent Neural Network) 등이 있습니다. 이러한 기술 발전을 바탕으로 최근에는 생성형 AI가 등장하여, 텍스트, 이미지, 음성 등 다양한 데이터를 직접 생성할 수 있게 되었습니다.

### 1.2. 생성형 AI와 LLM

생성형 AI(Generative AI)는 기존 데이터를 바탕으로 새로운 데이터를 생성하는 인공지능입니다. 대표적인 예로 LLM(Large Language Model, 대형 언어 모델)이 있습니다. LLM은 방대한 텍스트 데이터를 학습하여 자연스러운 문장 생성, 질의응답, 번역, 요약 등 다양한 언어 작업을 수행할 수 있습니다. GPT, BERT, LLaMA 등이 대표적인 LLM입니다.

LLM은 사전 학습(Pre-training)된 모델로, 학습 데이터에 없는 정보나 최신 정보에는 한계가 있습니다. 이를 보완하기 위해 파인튜닝(Fine-tuning)이나 외부 데이터베이스와의 연동이 필요합니다.

---

## 2. RAG(Retrieval-Augmented Generation)와 임베딩, 벡터 데이터베이스

### 2.1. RAG의 필요성과 구조

LLM은 학습 데이터에 기반하여 답변을 생성하기 때문에, 최신 정보 반영이나 도메인 특화 지식 활용에 한계가 있습니다. RAG(Retrieval-Augmented Generation)는 이러한 한계를 극복하기 위해, 외부 데이터베이스에서 정보를 검색(Retrieve)하여 답변 생성(Generate)에 활용하는 구조입니다.

RAG의 전체 동작 흐름은 다음과 같습니다.

1. 사용자가 텍스트를 입력합니다.
2. 입력 텍스트를 임베딩(Embedding) 과정을 통해 벡터(숫자 배열)로 변환합니다.
3. 벡터 데이터베이스(Vector DB)에서 유사한 문서를 검색합니다.
4. 검색된 문서를 LLM에 전달하여, 문서 기반의 답변을 생성합니다.

> **중요:** RAG는 LLM의 한계를 보완하여, 최신 정보나 도메인 특화 지식을 반영할 수 있습니다.

### 2.2. 임베딩과 벡터 데이터베이스

임베딩은 텍스트, 이미지 등 비정형 데이터를 고정된 크기의 벡터(숫자 배열)로 변환하는 과정입니다. 임베딩된 벡터는 의미적으로 유사한 데이터끼리 벡터 공간상에서 가까운 위치에 매핑됩니다. 예를 들어, "고양이"와 "강아지"는 임베딩 벡터 공간상에서 가까운 위치에 존재합니다.

벡터 데이터베이스(Vector DB)는 이러한 임베딩 벡터를 저장하고, 입력 벡터와의 유사도를 계산하여 관련 정보를 빠르게 검색할 수 있도록 지원합니다. 대표적인 벡터 DB로는 FAISS, ChromaDB 등이 있습니다.

#### 코드 예시: 코사인 유사도 계산

**제목:** 두 벡터의 코사인 유사도 계산

```python
import numpy as np  # 수치 연산 라이브러리 임포트

vec1 = np.array([1, 2, 3])  # 첫 번째 벡터
vec2 = np.array([4, 5, 6])  # 두 번째 벡터

def cosine_similarity(a, b):
    dot_product = np.dot(a, b)  # 두 벡터의 내적 계산
    norm_a = np.linalg.norm(a)  # 첫 번째 벡터의 크기(노름)
    norm_b = np.linalg.norm(b)  # 두 번째 벡터의 크기(노름)
    return dot_product / (norm_a * norm_b)  # 코사인 유사도 반환

similarity = cosine_similarity(vec1, vec2)
print(similarity)  # 출력: 0.9746318461970762
```

> **추가 정보:** 코사인 유사도는 임베딩 벡터 간 의미적 유사성을 측정하는 데 널리 사용됩니다.

---

## 3. 개발 환경 구축 및 주요 라이브러리

### 3.1. 가상환경의 필요성과 구축

생성형 AI 및 RAG 기반 애플리케이션을 개발할 때는 다양한 라이브러리와 도구가 필요하며, 이들 간의 버전 충돌이 자주 발생할 수 있습니다. 이를 방지하기 위해 프로젝트별로 가상환경을 구축하여 라이브러리 버전을 독립적으로 관리하는 것이 중요합니다.

#### 코드 예시: 가상환경 생성 및 활성화

**제목:** Conda로 가상환경 생성 및 활성화

```bash
conda create -n langchain_llm_env python=3.10  # 'langchain_llm_env'라는 이름의 파이썬 3.10 환경 생성
conda activate langchain_llm_env  # 생성한 가상환경 활성화
```

### 3.2. 주요 라이브러리 및 설치

- **openai**: OpenAI API 활용
- **langchain**: LLM, 벡터 DB, 외부 데이터 연동 등 파이프라인 구성
- **FAISS, ChromaDB**: 벡터 데이터베이스
- **python-dotenv**: 환경 변수(.env 파일) 관리
- **streamlit**: 웹 UI 개발
- **numpy, pandas, matplotlib, seaborn**: 데이터 분석 및 시각화

#### 코드 예시: 라이브러리 설치

**제목:** pip로 주요 라이브러리 설치

```bash
pip install openai==1.52.0
pip install langchain==0.2.16 langchain-core==0.2.38 langchain-community==0.2.16
pip install faiss-cpu==1.8.0 chromadb==0.5.5
pip install python-dotenv==1.0.0
pip install streamlit==1.27.0
pip install numpy pandas matplotlib seaborn
```

> **주의:** 라이브러리 버전 충돌이 발생할 경우, 해당 패키지를 삭제 후 원하는 버전으로 재설치해야 합니다.

### 3.3. 환경 변수 관리

API 키 등 민감한 정보는 코드에 직접 입력하지 않고, `.env` 파일이나 환경 변수로 관리해야 합니다.

#### 코드 예시: .env 파일에서 API 키 읽기

**제목:** .env 파일에서 환경 변수 읽기

```python
from dotenv import load_dotenv  # .env 파일에서 환경변수 읽기
import os  # 운영체제 환경변수 접근

load_dotenv()  # .env 파일의 내용을 환경변수로 로드
api_key = os.getenv('OPENAI_API_KEY')  # 환경변수에서 API 키 읽기
if not api_key:
    raise ValueError('환경변수가 설정되지 않았습니다!!')
print(api_key)  # 실제 서비스에서는 print 대신 안전하게 관리 필요
```

> **중요:** .env 파일은 깃허브 등 공개 저장소에 업로드하지 않도록 반드시 .gitignore에 추가해야 합니다.

---

## 4. OpenAI API 활용 및 Streamlit 대화형 요약 서비스 구현

### 4.1. OpenAI API 연동 및 모델 정보 확인

OpenAI API를 활용하려면 클라이언트 객체를 생성하고, 사용 가능한 모델 목록을 확인할 수 있습니다.

#### 코드 예시: OpenAI 클라이언트 생성 및 모델 조회

**제목:** OpenAI 클라이언트 생성 및 모델 리스트 출력

```python
import openai  # OpenAI 라이브러리 임포트
from openai import OpenAI  # OpenAI의 새로운 API 클라이언트

client = OpenAI(api_key=api_key)  # OpenAI 클라이언트 생성
models = client.models.list()  # 사용 가능한 모델 목록 조회

print('모델 개수:', len(models.data))  # 모델 개수 출력
print('모델 ID 목록:', [model.id for model in models.data])  # 모델 ID 리스트 출력
```

### 4.2. 주요 엔드포인트 및 기능

OpenAI API는 다양한 엔드포인트를 제공합니다. 각 엔드포인트의 용도와 특징을 이해하면 실제 서비스 개발에 큰 도움이 됩니다.

| 엔드포인트                | 주요 기능 설명                        |
|--------------------------|--------------------------------------|
| client.chat.completions  | 대화형 프롬프트(챗봇 등)             |
| client.completions       | 단일 텍스트 생성(비대화형)           |
| client.embeddings        | 텍스트 임베딩(숫자 벡터 변환)        |
| client.images            | 이미지 생성                          |
| client.audio.transcriptions | 음성 인식(STT), 음성 합성(TTS)    |

---

### 4.3. Streamlit을 활용한 대화형 요약 서비스 구현

Streamlit은 파이썬 기반의 웹 애플리케이션을 빠르게 만들 수 있는 프레임워크입니다. OpenAI API와 연동하여, 사용자가 입력한 텍스트를 요약해주는 간단한 웹 서비스를 구현할 수 있습니다.

#### 코드 예시: Streamlit 요약 서비스 전체 코드

**제목:** Streamlit과 OpenAI API를 활용한 요약 서비스

```python
import os  # 운영체제 기능 사용 (환경 변수 등)
import openai  # OpenAI API 호출용 라이브러리
import streamlit as st  # Streamlit 웹 앱 개발용
from openai import OpenAI  # OpenAI의 새로운 API 클라이언트
from dotenv import load_dotenv  # .env 파일에서 환경 변수 불러오기

def askChat(query, key):
    """
    사용자가 입력한 텍스트(query)를 OpenAI GPT 모델에 전달하여 응답을 받아오는 함수

    매개변수:
        query (str): 사용자 입력 텍스트 또는 질문
        key (str): OpenAI API 키

    반환값:
        str: GPT 모델의 응답 텍스트
    """
    client = OpenAI(api_key=key)  # OpenAI API 클라이언트 생성

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # 사용할 GPT 모델 지정
        messages=[{'role': 'user', 'content': query}]  # 사용자 메시지 전달
    )

    return response.choices[0].message.content  # 응답 텍스트 반환

def main():
    st.set_page_config(page_title="Chat with OpenAI", page_icon="💬")  # 페이지 설정

    if 'api_key' not in st.session_state:
        st.session_state['api_key'] = ''  # 세션 상태에 API 키 저장

    with st.sidebar:
        key = st.text_input(
            label='OpenAI Key',  # 입력 필드 라벨
            placeholder='api key',  # 입력 안내문
            value='',  # 기본값
            type='password'  # 비밀번호 입력 형태로 표시
        )
        if key:
            st.session_state['api_key'] = key  # 입력된 키를 세션 상태에 저장

    st.header('요약 응답')  # 본문 헤더 표시

    txt = st.text_area('글 입력')  # 사용자가 요약할 텍스트 입력

    if st.button('요약해줘'):
        st.info(askChat(txt, st.session_state['api_key']))  # 요약 결과 표시

if __name__ == "__main__":
    main()  # main 함수 실행
```

> **실행 결과:**  
> 1. 사이드바에서 OpenAI API 키를 입력합니다.  
> 2. 본문에 요약할 텍스트를 입력합니다.  
> 3. "요약해줘" 버튼을 클릭하면, GPT 모델이 생성한 요약 결과가 화면에 표시됩니다.

> **실무 팁:**  
> API 키는 반드시 환경 변수 또는 .env 파일로 관리하며, 코드에 직접 노출하지 않습니다.  
> Streamlit 앱 실행은 `streamlit run 파일명.py` 명령어로 진행합니다.

---

## 5. 임베딩 실습 및 LangChain을 활용한 RAG 시스템

### 5.1. 임베딩 실습

실제 텍스트를 임베딩 벡터로 변환하는 과정을 살펴봅니다.

#### 코드 예시: 텍스트 임베딩

**제목:** OpenAI API로 텍스트 임베딩 생성

```python
texts = [
    '아토는 너무 이쁜 강아지 입니다.',
    '이제부터는 초 겨울이네요',
    '고양이는 사랑스럽습니다'
]
print(texts)  # 입력 텍스트 리스트 출력

response = client.embeddings.create(
    model='text-embedding-3-small',  # 임베딩 모델 지정
    input=texts                      # 임베딩할 텍스트 리스트
)

print('embedding len - ', len(response.data[0].embedding))  # 임베딩 벡터 길이 출력
print('embedding value - ', response.data[0].embedding[:10])  # 임베딩 벡터 앞 10개 값 출력
# 출력 예시: embedding len - 1536, embedding value - [0.0123, -0.0456, ...]
```

### 5.2. LangChain + FAISS를 활용한 RAG 실습

LangChain은 LLM, 벡터 DB, 외부 데이터 연동 등 다양한 기능을 쉽게 연결해주는 프레임워크입니다. FAISS는 대규모 벡터 검색에 특화된 라이브러리입니다.

#### 코드 예시: LangChain 기반 RAG 시스템 구축

**제목:** LangChain과 FAISS를 활용한 RAG 질의응답

```python
from langchain.vectorstores import FAISS  # 벡터 데이터베이스
from langchain.embeddings.openai import OpenAIEmbeddings  # 임베딩 객체
from langchain.chains import RetrievalQA  # 검색 기반 질의응답 체인
from langchain.llms import OpenAI  # LLM 래퍼

# OpenAI LLM 객체 생성
client = OpenAI(api_key=api_key)

# 임베딩 객체 생성
embeddings = OpenAIEmbeddings()

# 테스트용 문서 리스트
docs = [
    {'content': '인공지능의 RNN을 기반으로 한 LLM은 RAG와 결합한 질의 응답 방식', 'metadata': {'source': 'doc1'}},
    {'content': 'cnn 과 rnn 차이점은 설명', 'metadata': {'source': 'doc2'}}
]

# 문서 내용만 추출하여 벡터 DB 생성
vectorDB = FAISS.from_texts([d['content'] for d in docs], embedding=embeddings)

# 벡터 DB에서 유사 문서 검색을 위한 Retriever 생성
retriever = vectorDB.as_retriever(search_kwargs={'k': 1})  # 가장 유사한 문서 1개 반환

# LLM과 Retriever를 결합한 질의응답 체인 생성
qa = RetrievalQA.from_chain_type(
    llm=OpenAI(model='gpt-4o-mini', temperature=0.9),  # 사용할 LLM과 옵션 지정
    chain_type='stuff',                                # 체인 타입(stuff, map_reduce 등)
    retriever=retriever                                # 벡터 검색기 연결
)

# 사용자의 질의 입력 및 RAG 기반 질의응답 실행
query = '인공지능'
answer = qa.run(query)
print('answer - ', answer)
# 출력 예시: answer - 인공지능의 RNN을 기반으로 한 LLM은 RAG와 결합한 질의 응답 방식입니다.
```

> **실무 팁:**  
> LangChain의 일부 클래스는 곧 폐기(deprecated)될 예정이므로, 최신 문서를 참고하여 사용해야 합니다.

---



## 주요 개념 요약표

| 개념                | 설명                                                         | 예시/비고                                          |
|---------------------|--------------------------------------------------------------|----------------------------------------------------|
| 인공지능(AI)        | 인간의 지능을 모방하는 컴퓨터 시스템                         | 챗봇, 이미지 인식 등                               |
| 머신러닝(ML)        | 데이터에서 패턴을 학습하여 예측/분류를 수행하는 AI의 하위 분야 | 주택 가격 예측, 스팸 메일 분류                     |
| 딥러닝(DL)          | 인공신경망을 활용한 머신러닝의 한 분야                        | CNN, RNN 등                                        |
| 생성형 AI(Gen AI)   | 기존 데이터를 바탕으로 새로운 데이터를 생성하는 AI            | 텍스트 요약, 이미지 생성                           |
| LLM                 | 대규모 텍스트 데이터로 학습한 언어 모델                      | GPT, BERT, LLaMA                                   |
| 임베딩(Embedding)   | 텍스트 등 비정형 데이터를 벡터(숫자 배열)로 변환하는 과정      | "고양이"와 "강아지" 임베딩 벡터가 가까움           |
| 벡터 데이터베이스   | 임베딩 벡터를 저장하고 유사도 기반 검색을 지원하는 DB         | FAISS, ChromaDB                                    |
| RAG                 | 외부 DB 검색과 LLM 생성을 결합한 구조                        | 최신 정보 반영 Q&A 시스템                          |
| LangChain           | LLM, 임베딩, 벡터 DB, 프롬프트 등 파이프라인 연결 프레임워크  | RAG 기반 질의응답 시스템 구축                      |
| Streamlit           | 파이썬 기반 웹 UI 프레임워크                                 | 대화형 요약 서비스, 챗봇 UI                        |
| .env, python-dotenv | 환경 변수 파일 및 관리 라이브러리                            | API 키 보안 관리                                   |

---