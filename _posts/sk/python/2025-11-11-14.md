---
title: "Rookies 14일차: Python - LLM, Langchain, RAG 2"
excerpt: "인공지능 서비스 구현을 위한 LLM, RAG, LangChain의 원리 및 실습"

categories: python
tags: [python, security, sk]

typora-root-url: ../../

date: 2025-11-11
last_modified_at: 2025-11-11
published: true
---

# # 학습 내용 요약 (2025-11-11)

이번 학습에서는 LLM(대형 언어 모델), RAG(검색 기반 생성), LangChain(연결 프레임워크)의 기본 원리와 실습 방법을 정리하였습니다. 단순한 모델 호출을 넘어, 외부 데이터베이스와 결합하여 최신 정보 기반의 답변을 생성하는 인공지능 서비스의 구조도 다루었습니다. 또한, 실무에서 자주 마주치는 보안 이슈, 환경 변수 관리, 문서 임베딩 및 검색, 그리고 Streamlit을 활용한 웹앱 구현까지 실제 활용 흐름에 맞춰 학습하였습니다.

---

## 1. LLM, RAG, LangChain: 현대 인공지능 서비스의 핵심 구조

### 1.1. LLM(Large Language Model)의 역할과 한계

LLM은 대규모 텍스트 데이터를 학습하여 자연어를 이해하고 생성하는 인공지능 모델입니다. 대표적으로 OpenAI의 GPT-3, GPT-4 등이 있으며, 사용자의 질문을 이해하고 적절한 답변을 생성하는 데 특화되어 있습니다.  
그러나 LLM은 학습 시점 이후의 최신 정보나 특정 도메인(예: 사내 문서, 논문 등)에 대한 지식이 부족할 수 있습니다. 이러한 한계를 극복하기 위해 RAG와 같은 구조가 필요합니다.

> **중요:** LLM은 자체적으로 방대한 지식을 내장하고 있지만, 최신성이나 특수한 데이터에 대한 접근성은 제한적입니다.

### 1.2. RAG(Retrieval-Augmented Generation)의 개념과 흐름

RAG는 LLM이 답변을 생성하기 전에, 외부 데이터베이스에서 관련 정보를 검색하여 참고하는 구조입니다.  
이 방식은 마치 LLM이 "도서관 사서"처럼 필요한 정보를 찾아보고, 그 결과를 바탕으로 답변을 생성하는 것과 유사합니다.

**RAG의 기본 흐름:**
1. 사용자의 질문 입력
2. 외부 DB(문서, 논문 등)에서 관련 정보 검색
3. 검색 결과를 LLM에 전달하여 답변 생성

이 구조를 통해 LLM은 최신 정보나 조직 내부의 지식까지 반영할 수 있습니다.

### 1.3. LangChain: 인공지능 파이프라인의 연결 프레임워크

LangChain은 LLM, RAG, 데이터베이스, 외부 API 등 다양한 컴포넌트를 연결하여 자동화된 인공지능 파이프라인을 쉽게 구축할 수 있도록 지원하는 프레임워크입니다.  
LLM 호출, 문서 검색, 체인 연결, 워크플로우 자동화 등 다양한 기능을 제공하며, 각 요소의 역할과 연결 방식을 명확히 이해하는 것이 중요합니다.

---

## 2. 실습 환경 준비 및 보안 관리

실제 인공지능 서비스를 개발할 때는 실습 환경의 구성과 보안이 매우 중요합니다.  
특히, OpenAI API Key 등 민감한 정보를 안전하게 관리하는 방법을 반드시 숙지해야 합니다.

### 2.1. 가상환경과 Jupyter Notebook 실행

가상환경을 사용하면 프로젝트별로 라이브러리와 파이썬 버전을 독립적으로 관리할 수 있습니다.  
아래는 아나콘다 환경에서 Jupyter Notebook을 실행하는 기본 명령어입니다.

```bash
conda env list  # 가상환경 목록 확인
conda activate <가상환경명>  # 원하는 가상환경 활성화
jupyter notebook  # Jupyter Notebook 실행
```
- 가상환경을 사용하면 라이브러리 버전 충돌을 예방할 수 있습니다.

### 2.2. 환경 변수와 .env 파일을 통한 API Key 관리

API Key 등 민감한 정보는 코드에 직접 입력하지 않고, 반드시 환경 변수 또는 `.env` 파일로 관리해야 합니다.

**제목:** 환경 변수에서 API Key 안전하게 불러오기
```python
import os  # 운영체제 기능 사용
from dotenv import load_dotenv  # .env 파일에서 환경변수 불러오기

load_dotenv()  # .env 파일의 환경변수 로드
api_key = os.getenv("OPENAI_API_KEY")  # 'OPENAI_API_KEY' 환경변수 값 반환
```

**제목:** API Key 마스킹 함수 구현
```python
def masking(key: str) -> str:
    """
    API Key 등 민감 정보를 앞 4자리, 뒤 4자리만 남기고 *로 마스킹합니다.
    - key: 마스킹할 문자열
    - 반환값: 마스킹된 문자열
    """
    if len(key) <= 8:
        return '*' * len(key)  # 전체를 마스킹
    return key[:4] + '*' * (len(key) - 8) + key[-4:]

masked_api_key = masking(api_key)
print('masked api key: ', masked_api_key)  # 예시 출력: abcd********5678
```
> **주의:** `.env` 파일은 절대 깃허브 등 외부에 노출하지 않아야 합니다.

---

## 3. LLM을 활용한 보안 체크리스트 생성 실습

LLM을 활용하여 패키지 설치, 모니터링, 민감 정보 관리 등 보안 체크리스트를 생성하는 예시를 실습하였습니다.

**제목:** LLM을 활용한 보안 체크리스트 생성 코드
```python
from openai import OpenAI  # OpenAI API 호출 라이브러리

client = OpenAI(api_key=api_key)  # API Key로 클라이언트 생성

system_content = '''
당신은 친절한 파이썬 보안 도우미입니다.
사용자의 요청에 대해 항상 보안 모범 사례를 우선으로 설명하고,
민감 정보 노출을 방지하는 방법, 최소 권한 원칙, 패키지/채널 검증, 파일 권한 설정,
취약점 완화 방법을 구체적 명령어와 체크리스트 형태로 제공하십시오.
응답에 실제 비밀번호나 실사용 API 키를 절대 포함하지 마십시오.
'''

user_content = '''
1) 패키지 설치 시 보안 지침
2) 모니터링 권장 설정 방법
3) 민감 정보 관리 방법과 예시
4) 가상환경 구축 권장 방법
'''

response = client.chat.completions.create(
    model="gpt-4o-mini",  # 경량화된 최신 모델
    messages=[
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content}
    ],
    max_tokens=512,      # 응답 최대 길이
    temperature=0.9      # 창의성(0~1, 높을수록 다양성)
)

print(response.choices[0].message.content)  # 생성된 답변 출력
```
- **system**: LLM의 역할과 응답 형식, 금지사항을 명확히 지정
- **user**: 실제 요청 내용

**실행 결과 예시(일부):**
```
1) 패키지 설치 시 보안 지침
- 공식 채널 사용: python -m pip install <패키지명>
- 패키지 검증: pip hash <패키지파일.whl>
- 최소 권한 원칙: python -m pip install --user <패키지명>
- 정기 업데이트: python -m pip list --outdated

2) 모니터링 권장 설정 방법
- 로그 모니터링 활성화, 알림 설정, 침입 탐지 시스템 설치 등

3) 민감 정보 관리 방법과 예시
- 환경 변수 사용, .env 파일 활용, 코드에 직접 입력 금지
```

---

## 4. RAG와 LangChain을 활용한 문서 기반 챗봇 실습

실제 도서관 사서 챗봇처럼, 사용자의 질문에 대해 외부 데이터베이스에서 정보를 검색하고, 그 결과를 참고하여 답변을 생성하는 과정을 실습하였습니다.

### 4.1. 문서 데이터 준비 및 분할

문서 데이터를 외부 DB 역할로 준비하고, LangChain의 텍스트 분할기를 사용하여 적절한 크기로 나눕니다.

**제목:** 문서 데이터 분할 코드
```python
from langchain.text_splitter import CharacterTextSplitter  # 텍스트 분할기

doc = [
    '리스트는 파이썬에서 변경 가능한(mutable) 자료형으로, 요소를 추가하거나 삭제할 수 있습니다.',
    '튜플은 변경 불가능한(immutable) 자료형으로, 한 번 생성하면 수정할 수 없습니다.',
    '딕셔너리는 키(key)와 값(value)의 쌍으로 데이터를 저장합니다.'
]

text_splitter = CharacterTextSplitter(chunk_size=200)  # 200자 단위로 분할
texts = text_splitter.create_documents(doc)  # 분할된 문서 리스트 반환
print(texts)  # 분할 결과 확인
# 출력: [Document(page_content='리스트는 ...'), ...]
```
- 긴 문서를 일정 크기로 나누어 검색 및 임베딩 효율을 높입니다.

### 4.2. 임베딩 및 벡터DB 구축

문서를 임베딩(숫자 벡터로 변환)하여 FAISS 벡터DB에 저장합니다.

**제목:** 문서 임베딩 및 벡터DB 구축 코드
```python
from langchain.vectorstores import FAISS  # 벡터DB
from langchain.embeddings.openai import OpenAIEmbeddings  # OpenAI 임베딩

embeddings = OpenAIEmbeddings(model='text-embedding-3-small')  # 임베딩 모델 생성
db = FAISS.from_documents(texts, embedding=embeddings)  # 벡터DB에 저장
print(db)  # 벡터DB 객체 확인
```

### 4.3. 검색기(Retriever) 설정

질문에 대해 가장 유사한 문서를 검색하는 검색기를 생성합니다.

**제목:** 검색기 생성 코드
```python
retriever = db.as_retriever(search_kwargs={'k': 1})  # k: 반환할 문서 개수
print(retriever)  # 검색기 정보 확인
```

### 4.4. LLM과 검색기 결합: 질의응답 체인

LLM과 검색기를 결합하여, 검색 기반 질의응답(RAG)을 수행합니다.

**제목:** 질의응답 체인 생성 코드
```python
from langchain.chains import RetrievalQA  # 검색 기반 QA 체인
from langchain.chat_models import ChatOpenAI  # LLM

qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model='gpt-4o-mini', temperature=0.9),  # 사용할 LLM과 옵션
    chain_type='stuff',  # 체인 방식(stuff, map_reduce 등)
    retriever=retriever  # 검색기 연결
)
print(qa)  # 체인 정보 확인
```

### 4.5. 실제 질의응답 실행

사용자의 질문에 대해, 검색 기반 LLM 답변을 생성합니다.

**제목:** 질의응답 실행 및 결과 확인 코드
```python
query = '파이썬 리스트와 튜플의 차이점을 설명해줘'  # 예시 질문
answer = qa.run(query)  # 답변 생성

print('Q - ', query)
print('사서가 참고한 내용 - ', retriever.get_relevant_documents(query)[0].page_content)
print('answer - ', answer)
# 출력:
# Q -  파이썬 리스트와 튜플의 차이점을 설명해줘
# 사서가 참고한 내용 -  리스트는 파이썬에서 변경 가능한(mutable) 자료형...
# answer -  파이썬 리스트와 튜플의 주요 차이점은 다음과 같습니다: ...
```
- 검색된 문서와 LLM이 생성한 답변을 함께 확인할 수 있습니다.

---

## 5. 실무에서의 주의사항 및 실습 팁

- 민감 정보(API Key 등)는 반드시 환경 변수 또는 `.env` 파일로 관리합니다.
- 공식 저장소(Pypi 등)만 이용하고, 설치 전 해시값 검증을 습관화합니다.
- 프로젝트별로 가상환경(venv, conda 등)을 생성하여, 패키지 충돌을 방지합니다.
- LangChain 등 주요 라이브러리는 버전 업데이트가 잦으므로, 공식 문서와 버전 호환성을 반드시 확인해야 합니다.
- Streamlit 등 웹앱 프레임워크를 활용하면, 챗봇이나 대시보드를 손쉽게 구현할 수 있습니다.

---

## 전체 흐름 요약

이번 학습에서는 LLM, RAG, LangChain의 핵심 개념과 실무 활용 흐름을 체계적으로 정리하였습니다.  
실제 코드를 통해 문서 임베딩, 벡터DB 구축, 검색 기반 질의응답 체인 구현, 그리고 보안 체크리스트 생성까지 실습하였습니다.  
실무에서 자주 발생하는 보안 이슈와 환경 변수 관리, 그리고 최신 라이브러리 버전 관리의 중요성도 함께 다루었습니다.

---

## 주요 개념 요약표

| 개념        | 설명                                                         | 예시/비고                                 |
|-------------|--------------------------------------------------------------|-------------------------------------------|
| LLM         | 대규모 데이터를 학습한 자연어 이해·생성 인공지능 모델        | GPT-3, GPT-4 등                           |
| RAG         | 검색 기반 생성. 외부 DB에서 정보 검색 후 LLM이 답변 생성     | 도서관 사서 챗봇, 사내 Q&A                |
| LangChain   | LLM, RAG, DB, API 등 연결하는 파이프라인 프레임워크          | 체인, 워크플로우 자동화                   |
| 임베딩      | 텍스트를 숫자 벡터로 변환하는 과정                           | OpenAIEmbeddings, text-embedding-3-small  |
| 벡터DB      | 임베딩된 벡터를 저장·검색하는 데이터베이스                   | FAISS, Chroma 등                          |
| 검색기      | 질문과 유사한 문서를 벡터DB에서 검색하는 기능                | as_retriever(search_kwargs={'k': 1})      |
| RetrievalQA | 검색 결과와 LLM을 결합한 질의응답 체인                       | chain_type='stuff', map_reduce 등         |
| 환경 변수   | 민감 정보(API Key 등) 안전하게 관리하는 방법                 | .env 파일, os.getenv()                    |
| 마스킹      | 민감 정보 노출 방지 위해 일부만 표시                         | abcd********5678                          |
| 가상환경    | 프로젝트별 라이브러리·파이썬 버전 독립 관리                  | conda, venv, requirements.txt             |
| Streamlit   | 파이썬 기반 웹앱 프레임워크                                  | 대시보드, 챗봇 UI 구현                     |

---

## 공부 후기

오늘은 LLM, RAG, LangChain의 기본 개념부터 실무에 적용하는 방법까지 체계적으로 정리하였다. 단순히 모델을 호출하는 수준을 넘어, 외부 데이터와 결합하여 실제로 쓸 수 있는 인공지능 서비스를 만드는 전체 흐름을 이해할 수 있었다. 특히, 환경 변수 관리와 보안 실수 방지, 그리고 문서 임베딩과 벡터DB 구축 과정이 실제 프로젝트에 매우 중요하다는 점을 다시 한 번 실감했다. 앞으로는 다양한 데이터와 도메인에 맞게 RAG 구조를 확장하고, 실무에서 바로 활용할 수 있도록 꾸준히 연습할 계획이다.